{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS ###############################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,TensorBoard,ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Activation, Conv2D, MaxPool2D, BatchNormalization, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import *\n",
    "\n",
    "import skimage\n",
    "from skimage.transform import rescale, resize\n",
    "\n",
    "import pydot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS ###############################\n",
    "\n",
    "EPOCHS = 50\n",
    "BS = 128\n",
    "DROPOUT_RATE = 0.5\n",
    "FROZEN_LAYER_NUM = 19\n",
    "\n",
    "ADAM_LEARNING_RATE = 0.001\n",
    "SGD_LEARNING_RATE = 0.01\n",
    "SGD_DECAY = 0.0001\n",
    "\n",
    "img_width, img_height = 197,197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET ##########################\n",
    "\n",
    "#folder where we will save the logs and the model\n",
    "folder='C:/Users/Maryem/Desktop/P2M/logs/VGG16'\n",
    "\n",
    "train_dataset='C:/Users/Maryem/Desktop/fer2013/train.csv'\n",
    "eval_dataset \t= 'C:/Users/Maryem/Desktop/fer2013/fer2013_eval.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL #####################################\n",
    "\n",
    "#create the base pre-trained resnet50 model\n",
    "    #include_top: include the top fully connected layer --> set to false\n",
    "    #weights: pre-training on imagenet\n",
    "    #input_shape: the shape of the input image\n",
    "\n",
    "vgg16 = tf.keras.applications.VGG16(include_top=False,weights='imagenet',input_shape=(img_width, img_height, 3), pooling='avg')\n",
    "#we take last_layer the output of our pre-trained model\n",
    "last_layer = vgg16.get_layer('global_average_pooling2d').output\n",
    "#add a flatten layer\n",
    "x = Flatten(name='flatten')(last_layer)\n",
    "#add a dropout layer \n",
    "\n",
    "x = Dropout(DROPOUT_RATE)(x)\n",
    "\n",
    "#add a fully connected layer with the activation function relu\n",
    "x = Dense(4096, activation='relu', name='fc6')(x)\n",
    "x = Dropout(DROPOUT_RATE)(x)\n",
    "x = Dense(1024, activation='relu', name='fc7')(x)\n",
    "\n",
    "\n",
    "#freezing layers\n",
    "for i in range(FROZEN_LAYER_NUM):\n",
    "    vgg16.layers[i].trainable = False\n",
    "\n",
    "#defining the output layer\n",
    "    #activation function: softmax\n",
    "    #7 units for the 7 classes\n",
    "out = Dense(7, activation='softmax', name='classifier')(x)\n",
    "\n",
    "#defining the final model to be trained\n",
    "model = Model(vgg16.input, out)\n",
    "\n",
    "#defining the optimizer\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=ADAM_LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "#optim = keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=SGD_LEARNING_RATE, momentum=0.9, nesterov=True)\n",
    "rlrop = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc',mode='max',factor=0.5, patience=10, min_lr=0.00001, verbose=1)\n",
    "\n",
    "#compiling\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PREPARATION ###############################\n",
    "\n",
    "#create an ImageDataGenerator to generate batches of data\n",
    "# def get_datagen(dataset, aug=False):\n",
    "#     if aug:\n",
    "#         datagen = ImageDataGenerator(\n",
    "#                             rescale=1./255,\n",
    "#                             featurewise_center=False,\n",
    "#                             featurewise_std_normalization=False,\n",
    "#                             rotation_range=10,\n",
    "#                             width_shift_range=0.1,\n",
    "#                             height_shift_range=0.1,\n",
    "#                             zoom_range=0.1,\n",
    "#                             horizontal_flip=True)\n",
    "#     else:\n",
    "#         datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#     return datagen.flow_from_directory(\n",
    "#             dataset,\n",
    "#             target_size=(197, 197),\n",
    "#             color_mode='rgb',\n",
    "#             shuffle = True,\n",
    "#             class_mode='categorical',\n",
    "#             batch_size=BS)\n",
    "    \n",
    "# train_generator  = get_datagen('C:/Users/Maryem/Desktop/fer2013/train', True)\n",
    "# dev_generator    = get_datagen('C:/Users/Maryem/Desktop/fer2013/fer2013_eval.csv')\n",
    "# test_generator  = get_datagen('C:/Users/Maryem/Desktop/fer2013/test')\n",
    "# Preprocesses a numpy array encoding a batch of images\n",
    "    # x: Input array to preprocess\n",
    "def preprocess_input(x):\n",
    "    x -= 128.8006 # np.mean(train_dataset)\n",
    "    return x\n",
    "\n",
    "# Function that reads the data from the csv file, increases the size of the images and returns the images and their labels\n",
    "    # dataset: Data path\n",
    "def get_data(dataset):\n",
    "    file_stream = file_io.FileIO(dataset, mode='r')\n",
    "    data = pd.read_csv(file_stream)\n",
    "    pixels = data['pixels'].tolist()\n",
    "    images = np.empty((len(data), img_height, img_width, 3))\n",
    "    i = 0\n",
    "\n",
    "    for pixel_sequence in pixels:\n",
    "        single_image = [float(pixel) for pixel in pixel_sequence.split(' ')]  # Extraction of each single\n",
    "        single_image = np.asarray(single_image).reshape(48, 48) # Dimension: 48x48\n",
    "        single_image = resize(single_image, (img_height, img_width), order = 3, mode = 'constant') # Dimension: 139x139x3 (Bicubic)\n",
    "        ret = np.empty((img_height, img_width, 3))  \n",
    "        ret[:, :, 0] = single_image\n",
    "        ret[:, :, 1] = single_image\n",
    "        ret[:, :, 2] = single_image\n",
    "        images[i, :, :, :] = ret\n",
    "        i += 1\n",
    "    \n",
    "    images = preprocess_input(images)\n",
    "    labels = to_categorical(data['emotion'])\n",
    "\n",
    "    return images, labels    \n",
    "\n",
    "# Data preparation\n",
    "train_data_x, train_data_y  = get_data(train_dataset)\n",
    "val_data  = get_data(eval_dataset)\n",
    "\n",
    "# Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely\n",
    "# rescale:          Rescaling factor (defaults to None). Multiply the data by the value provided (before applying any other transformation)\n",
    "# rotation_range:   Int. Degree range for random rotations\n",
    "# shear_range:      Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
    "# zoom_range:       Float or [lower, upper]. Range for random zoom. If a float, [lower, upper] = [1-zoom_range, 1+zoom_range]\n",
    "# fill_mode :       Points outside the boundaries of the input are filled according to the given mode: {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}\n",
    "# horizontal_flip:  Boolean. Randomly flip inputs horizontally\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range  = 10,\n",
    "    shear_range     = 10, # 10 degrees\n",
    "    zoom_range      = 0.1,\n",
    "    fill_mode       = 'reflect',\n",
    "    horizontal_flip = True)\n",
    "\n",
    "# Takes numpy data & label arrays, and generates batches of augmented/normalized data. Yields batcfillhes indefinitely, in an infinite loop\n",
    "    # x:            Data. Should have rank 4. In case of grayscale data, the channels axis should have value 1, and in case of RGB data, \n",
    "    #               it should have value 3\n",
    "    # y:            Labels\n",
    "    # batch_size:   Int (default: 32)\n",
    "train_generator = train_datagen.flow(\n",
    "    train_data_x,\n",
    "    train_data_y,\n",
    "    batch_size  = BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TRAINING #######################################\n",
    "\n",
    "tensorboard= TensorBoard(\n",
    "    log_dir=folder+'/logs',\n",
    "    histogram_freq=0,\n",
    "    write_graph= True,\n",
    "    write_images= True)\n",
    "\n",
    "reduce_lr= ReduceLROnPlateau(\n",
    "    monitor = 'val_loss',\n",
    "    factor= 0.4,\n",
    "    patience = 5,\n",
    "    mode = 'auto',\n",
    "    min_lr= 1e-6)\n",
    "\n",
    "early_stop=EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 20,\n",
    "    mode = 'auto')\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    x = train_generator,\n",
    "    validation_data=val_data, \n",
    "    steps_per_epoch=len(train_data_x)// BS,\n",
    "    shuffle=True,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[tensorboard,reduce_lr,early_stop],\n",
    ") \n",
    "\n",
    "# print('\\n# Evaluate on dev data')\n",
    "# results_dev = mode.evaluate_generator(dev_generator, 3509 // BS)\n",
    "# # print('dev loss, dev acc:', results_dev)\n",
    "\n",
    "# # print('\\n# Evaluate on test data')\n",
    "# results_test = model.evaluate_generator(test_generator, 3509 // BS)l\n",
    "# print('test loss, test acc:', results_test)\n",
    "\n",
    "# # list all data in history\n",
    "# print(history.history.keys())\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'dev'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'dev'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# epoch_str = '-EPOCHS_' + str(EPOCHS)\n",
    "# test_acc = 'test_acc_%.3f' % results_test[1]\n",
    "\n",
    "model.save('VGG16.keras')\n",
    "\n",
    "with file_io.FileIO('VGG16.keras', mode='rb') as input_f:\n",
    "    with file_io.FileIO(folder + '/VGG16.keras', mode='wb') as output_f:  # w+ : writing and reading\n",
    "        output_f.write(input_f.read())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
